{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from Levenshtein import distance, jaro_winkler\n",
    "from pyjarowinkler import distance\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/Valeriya/Documents/Py/Old-High-German/ahd-texts/texts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('/Users/Valeriya/Downloads/prefixy_c_gi.csv').read()\n",
    "prefs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTexts(path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(path):\n",
    "        if \"idea\" not in filename and \".txt\" in filename:\n",
    "            with open(path + filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "def removeTagged(texts):\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        new_text = re.sub(\"<.+?>.+?</.+?>\", \"\", text)\n",
    "        new_texts.append(new_text)\n",
    "    return (new_texts)\n",
    "\n",
    "def tokenizeTexts(texts):\n",
    "    tokenized_texts = []\n",
    "    stop_sybols = \"№1234567890\"\n",
    "    stop_words = [\"page\", \"sentence\", \"verse\"]\n",
    "    for text in texts:\n",
    "        tokens = []\n",
    "        for word in text.split():\n",
    "            if not any(st in word for st in stop_sybols):\n",
    "                word = word.lower()\n",
    "                word = re.sub(\"[\\«\\»\\!<>\\?,\\.\\(\\)\\[\\]\\\";:\\\\/]\", \"\", word)\n",
    "                word = re.sub(\"_\", \" \", word)\n",
    "                if len(word) > 0 and word not in stop_words:\n",
    "                    tokens.append(word)\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "texts = getTexts(path)\n",
    "texts = removeTagged(texts)\n",
    "tokenized_texts = tokenizeTexts(texts)\n",
    "\n",
    "tokenized_texts_flat = [item for sublist in tokenized_texts for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = open('/Users/Valeriya/Documents/Py/Old-High-German/ahd-dictionary/ohg_dict_3.html')\n",
    "dictionary = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_dict = []\n",
    "for txt in dictionary.find_all('strong'):\n",
    "    tokenized_dict.append(txt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenized_texts_flat\n",
    "# tokenized_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "f = re.split(';', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in f:\n",
    "    n = re.sub('\\\\n', '', i)\n",
    "    if n != '':\n",
    "        prefs.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clear_dict = []\n",
    "clear_dict_dict = defaultdict()\n",
    "for i, lemma_raw in enumerate(set(tokenized_dict)):\n",
    "    if any(i in lemma_raw for i in prefs) == False:\n",
    "#         print(lemma_raw)\n",
    "#     if lemma_raw == 'muater':\n",
    "#         print('_________')\n",
    "        clear_dict_dict[lemma_raw] = lemma_raw\n",
    "    else:\n",
    "        for l in prefs:\n",
    "            if l in lemma_raw:\n",
    "                lemma = lemma_raw.strip(l)\n",
    "    #             clear_dict.append(lemma)\n",
    "                clear_dict_dict[lemma] = lemma_raw\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del clear_dict_dict['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear_dict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear_dict = [i for i in clear_dict if i != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(i in 'muoter' for i in prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clear_tokenized_texts_flat = []\n",
    "clear_tokenized_texts_dict = defaultdict()\n",
    "for i, lemma_raw in enumerate(set(tokenized_texts_flat)):\n",
    "    if any(i in lemma_raw for i in prefs) == False:\n",
    "#         print(lemma_raw)\n",
    "#     if lemma_raw == 'muater':\n",
    "#         print('_________')\n",
    "        clear_tokenized_texts_dict[lemma_raw] = lemma_raw\n",
    "    else:\n",
    "        for l in prefs:\n",
    "            if l in lemma_raw:\n",
    "                lemma = lemma_raw.strip(l)\n",
    "                flag = 1\n",
    "                clear_tokenized_texts_dict[lemma] = lemma_raw\n",
    "                break\n",
    "# #             clear_tokenized_texts_flat.append(lemma)\n",
    "#         else:\n",
    "#             continue\n",
    "#             clear_dict.append(lemma_raw)\n",
    "#     print(flag)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del clear_tokenized_texts_dict['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear_tokenized_texts_flat = [i for i in clear_tokenized_texts_flat if i != '']\n",
    "\n",
    "# for i in set(tokenized_texts_flat):\n",
    "#     if i =='muater':\n",
    "#         print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear_tokenized_texts_dict['isaias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = defaultdict(list)\n",
    "clusters_2 = []\n",
    "\n",
    "list_clear_dict = list(set(clear_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for word in tqdm_notebook(list(set(clear_tokenized_texts_flat))):\n",
    "#     for i, lemma in enumerate(list_clear_dict):\n",
    "#         dist = distance.get_jaro_distance(word, lemma, winkler=False)\n",
    "#         if dist > 0.85:\n",
    "#             clusters[list_clear_dict[i]].append(word)\n",
    "#     \"#     if word in variants_dict.keys():\\n\",\n",
    "#     \"#         clusters[tokenized_dict[min_index]].append(variants_dict[word])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for word in tqdm_notebook(list(clear_tokenized_texts_dict.keys())):\n",
    "    for i, lemma in enumerate(list(clear_dict_dict.keys())):\n",
    "#         dist = distance.get_jaro_distance(word, lemma, winkler=False)\n",
    "        dist = distance(word, lemma)\n",
    "        if dist < 2:\n",
    "    #             word_dictionary = clear_dict_dict[]\n",
    "            clusters[clear_dict_dict[lemma]].append(clear_tokenized_texts_dict[word])\n",
    "#         else:\n",
    "#             if dist == 2:\n",
    "#                 clusters[clear_dict_dict[lemma]].append(clear_tokenized_texts_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters['bechantnusse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from Levenshtein import distance\n",
    "distance('breste', 'breste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sorted(prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = defaultdict(list)\n",
    "list_tokenized_texts_flat = list(set(tokenized_texts_flat))\n",
    "list_tokenized_dict = list(set(tokenized_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for word in tqdm_notebook(list_tokenized_texts_flat):\n",
    "    for i, lemma in enumerate(list_tokenized_dict):\n",
    "#         dist = distance.get_jaro_distance(word, lemma, winkler=False)\n",
    "        dist = distance(word, lemma)\n",
    "        if len(word) < 4 and len(lemma) <4:\n",
    "            if dist < 2:\n",
    "        #             word_dictionary = clear_dict_dict[]\n",
    "                clusters[list_tokenized_dict[i]].append(word)\n",
    "        else:\n",
    "            if dist < 4:\n",
    "        #             word_dictionary = clear_dict_dict[]\n",
    "                clusters[list_tokenized_dict[i]].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geiste', 'gebes', 'gebet', 'gibíete']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters['gebreste']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "cf508759621d442aaa0ae05c7e269dc4": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "e3f52d48a25f4e45b81594d856946110": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
